{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def initialize(dim1, dim2):\n",
    "    \"\"\"\n",
    "    :param dim: size of vector w initilazied with zeros\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim1, dim2))\n",
    "    b = np.zeros(shape=(27, 1))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = X.shape[1]  # getting no of rows\n",
    "\n",
    "    # Forward Prop\n",
    "    A = softmax((np.dot(w.T, X) + b).T)\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A))\n",
    "\n",
    "    # backwar prop\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :param num_iters: number of iterations for gradient\n",
    "    :param alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if i>500:\n",
    "            alpha=alpha*0.9\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    :param w:\n",
    "    :param b:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # m = X.shape[1]\n",
    "    # y_pred = np.zeros(shape=(1, m))\n",
    "    # w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return y_pred\n",
    "\n",
    "def predict_for_cv(w, b, X):\n",
    "    \"\"\"\n",
    "    :param w:\n",
    "    :param b:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # m = X.shape[1]\n",
    "    # y_pred = np.zeros(shape=(1, m))\n",
    "    # w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return int(y_pred[0])\n",
    "\n",
    "def model(X_train, Y_train, Y,X_test,Y_test, num_iters, alpha, print_cost):\n",
    "    \"\"\"\n",
    "    :param X_train:\n",
    "    :param Y_train:\n",
    "    :param X_test:\n",
    "    :param Y_test:\n",
    "    :param num_iterations:\n",
    "    #completed\n",
    "    :param learning_rate:\n",
    "    :param print_cost:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    w, b = initialize(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)\n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "\n",
    "    y_prediction_train = predict(w, b, X_train)\n",
    "    y_prediction_test = predict(w, b, X_test)\n",
    "    print(\"Train accuracy: {} %\", sum(y_prediction_train == Y) / (float(len(Y))) * 100)\n",
    "    print(\"Test accuracy: {} %\", sum(y_prediction_test == Y_test) / (float(len(Y_test))) * 100)\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": y_prediction_test,\n",
    "         \"Y_prediction_train\": y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": alpha,\n",
    "         \"num_iterations\": num_iters}\n",
    "\n",
    "    # Plot learning curve (with costs)\n",
    "    #costs = np.squeeze(d['costs'])\n",
    "    #plt.plot(costs)\n",
    "    #plt.ylabel('cost')\n",
    "    #plt.xlabel('iterations (per hundreds)')\n",
    "    #plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "    #plt.plot()\n",
    "    #plt.show()\n",
    "    #plt.close()\n",
    "\n",
    "    #pri(X_test.T, y_prediction_test)\n",
    "    return d\n",
    "\n",
    "\n",
    "def pri(X_test, y_prediction_test):\n",
    "    example = X_test[2, :]\n",
    "    print(\"Prediction for the example is \", y_prediction_test[2])\n",
    "    plt.imshow(np.reshape(example, [28, 28]))\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
